"use strict";(self.webpackChunkstarlake_docs=self.webpackChunkstarlake_docs||[]).push([[7698],{3905:function(e,t,a){a.d(t,{Zo:function(){return p},kt:function(){return f}});var i=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,i)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,i,n=function(e,t){if(null==e)return{};var a,i,n={},r=Object.keys(e);for(i=0;i<r.length;i++)a=r[i],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)a=r[i],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var l=i.createContext({}),u=function(e){var t=i.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=u(e.components);return i.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},d=i.forwardRef((function(e,t){var a=e.components,n=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=u(a),f=n,m=d["".concat(l,".").concat(f)]||d[f]||c[f]||r;return a?i.createElement(m,o(o({ref:t},p),{},{components:a})):i.createElement(m,o({ref:t},p))}));function f(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var r=a.length,o=new Array(r);o[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:n,o[1]=s;for(var u=2;u<r;u++)o[u]=a[u];return i.createElement.apply(null,o)}return i.createElement.apply(null,a)}d.displayName="MDXCreateElement"},1965:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return o},contentTitle:function(){return s},metadata:function(){return l},toc:function(){return u},default:function(){return c}});var i=a(7462),n=a(3366),r=(a(7294),a(3905)),o={sidebar_position:1},s="Introduction",l={unversionedId:"userguide/introduction",id:"userguide/introduction",isDocsHomePage:!1,title:"Introduction",description:"The purpose of this project is to efficiently ingest various data",source:"@site/docs/userguide/introduction.md",sourceDirName:"userguide",slug:"/userguide/introduction",permalink:"/starlake/docs/userguide/introduction",editUrl:"https://github.com/starlake-ai/starlake/edit/master/docs/docs/userguide/introduction.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"cometSidebar",previous:{title:"What is it ?",permalink:"/starlake/docs/intro"},next:{title:"Quick Start",permalink:"/starlake/docs/userguide/quickstart"}},u=[{value:"How it works",id:"how-it-works",children:[{value:"On Premise Data Pipeline",id:"on-premise-data-pipeline",children:[]},{value:"Azure Databricks Data Pipeline",id:"azure-databricks-data-pipeline",children:[]},{value:"Data Pipeline on Google Cloud Storage",id:"data-pipeline-on-google-cloud-storage",children:[]},{value:"Data Pipeline on BigQuery",id:"data-pipeline-on-bigquery",children:[]}]}],p={toc:u};function c(e){var t=e.components,o=(0,n.Z)(e,["components"]);return(0,r.kt)("wrapper",(0,i.Z)({},p,o,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"The purpose of this project is to efficiently ingest various data\nsources in different formats and make them available for analytics.\nUsually, ingestion is done by writing hand made custom parsers that\ntransform input files into datasets of records."),(0,r.kt)("p",null,"This project aims at automating this parsing task by making data\ningestion purely declarative."),(0,r.kt)("p",null,"The workflow below is a typical use case :"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Export your data as a set of DSV (Delimiter-separated values) or JSON or XML files"),(0,r.kt)("li",{parentName:"ul"},"Define the structure of each DSV/JSON/XML file with a schema using YAML syntax"),(0,r.kt)("li",{parentName:"ul"},"Configure the ingestion process"),(0,r.kt)("li",{parentName:"ul"},"Start watching your data being available as Tables in your Data Factory")),(0,r.kt)("p",null,"The main advantages of Comet Data Pipeline are that it:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Eliminates manual coding for data ingestion"),(0,r.kt)("li",{parentName:"ul"},"Assign metadata to each dataset"),(0,r.kt)("li",{parentName:"ul"},"Expose data ingestion metrics and history"),(0,r.kt)("li",{parentName:"ul"},"Transform text files to strongly typed records without coding"),(0,r.kt)("li",{parentName:"ul"},"Support semantic types"),(0,r.kt)("li",{parentName:"ul"},"Apply privacy to specific fields"),(0,r.kt)("li",{parentName:"ul"},"very, very simple piece of software to administer")),(0,r.kt)("h2",{id:"how-it-works"},"How it works"),(0,r.kt)("p",null,"Comet Data Pipeline automates the loading and parsing of files and\ntheir ingestion into a Data Factory where datasets become\navailable as strongly typed records."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Complete Comet Data pipeline Workflow",src:a(2833).Z,title:"Complete Comet Data pipeline Workflow"})),(0,r.kt)("p",null,"The figure above describes how Comet implements the Extract Load Transform (ELT) Data Pipeline steps.\nComet may be used indistinctly for all or any of these steps."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},'The "extract" step allows to export selective data from an existing SQL database to a set of CSV / JSON / XML files.'),(0,r.kt)("li",{parentName:"ul"},'The "load" step allows to load text files, to ingest them as strong typed records stored as parquet files or DWH tables (eq. Google BigQuery)'),(0,r.kt)("li",{parentName:"ul"},'The "transform" step allows to join loaded data and save them as parquet files, DWH tables or Elasticsearch indices')),(0,r.kt)("p",null,"The Load Transform steps support multiple configurations for inputs and outputs as illustrated in the\nexamples below. "),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Anywhere",src:a(637).Z,title:"Anywhere"})),(0,r.kt)("p",null,"They all follow the same process :"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Landing Area : In this optional step, files with predefined pattern filenames are stored on a local filesystem in a predefined folder hierarchy"),(0,r.kt)("li",{parentName:"ul"},"Pending Area : Files associated with a schema are imported into the data factory."),(0,r.kt)("li",{parentName:"ul"},"Working Area : Pending files are parsed against their schema and records are rejected or accepted and made available in parquet files as Hive Tables or Big Query tables or parquet files in a cloud bucket."),(0,r.kt)("li",{parentName:"ul"},"Business Area : Tables (Hive / BigQuery / Parquet files) in the working area may be joined to provide a hoslictic view of the data through the definition of transformation."),(0,r.kt)("li",{parentName:"ul"},"Data visualization : parquet files / tables may be exposed in datawarehouses or elasticsearch indices through an indexing definition")),(0,r.kt)("p",null,"Input file schemas, ingestion rules, transformation and indexing definitions used in the steps above are all defined in YAML files."),(0,r.kt)("h3",{id:"on-premise-data-pipeline"},"On Premise Data Pipeline"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"On Premise Workflow",src:a(966).Z})),(0,r.kt)("h3",{id:"azure-databricks-data-pipeline"},"Azure Databricks Data Pipeline"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Azure Workflow",src:a(9235).Z})),(0,r.kt)("h3",{id:"data-pipeline-on-google-cloud-storage"},"Data Pipeline on Google Cloud Storage"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Cloud Storage Workflow",src:a(741).Z})),(0,r.kt)("h3",{id:"data-pipeline-on-bigquery"},"Data Pipeline on BigQuery"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Bigquery Workflow",src:a(9693).Z})))}c.isMDXComponent=!0},637:function(e,t,a){t.Z=a.p+"assets/images/anywhere-dfb894bb9ae7dd3d5b8738e7adf33349.png"},9235:function(e,t,a){t.Z=a.p+"assets/images/elt-azure-databricks-d9a816edc2b87547a519f7b3d358c86c.png"},9693:function(e,t,a){t.Z=a.p+"assets/images/elt-gcp-bq-26f9271ea7186b320a042f5710dcbb74.png"},741:function(e,t,a){t.Z=a.p+"assets/images/elt-gcp-gcs-8c3875d102d559d9f0239b4bb6fa5ac1.png"},966:function(e,t,a){t.Z=a.p+"assets/images/elt-onpremise-bb22b6b5e8a043d4eccb8d185e6034c5.png"},2833:function(e,t,a){t.Z=a.p+"assets/images/workflow-16367736a7bea45138f2cd57a13130aa.png"}}]);