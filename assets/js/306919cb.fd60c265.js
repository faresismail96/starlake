"use strict";(self.webpackChunkstarlake_docs=self.webpackChunkstarlake_docs||[]).push([[9825],{3905:function(e,t,a){a.d(t,{Zo:function(){return p},kt:function(){return f}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),m=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},p=function(e){var t=m(e.components);return n.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),c=m(a),f=r,k=c["".concat(s,".").concat(f)]||c[f]||d[f]||i;return a?n.createElement(k,l(l({ref:t},p),{},{components:a})):n.createElement(k,l({ref:t},p))}));function f(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,l=new Array(i);l[0]=c;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:r,l[1]=o;for(var m=2;m<i;m++)l[m]=a[m];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},4480:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return l},contentTitle:function(){return o},metadata:function(){return s},toc:function(){return m},default:function(){return d}});var n=a(7462),r=a(3366),i=(a(7294),a(3905)),l={sidebar_position:1,title:"Configuration"},o=void 0,s={unversionedId:"reference/configuration",id:"reference/configuration",isDocsHomePage:!1,title:"Configuration",description:"Comet Data Pipeline is written using Scala / Spark and is thus run using the spark-submit command.",source:"@site/docs/reference/configuration.md",sourceDirName:"reference",slug:"/reference/configuration",permalink:"/starlake/docs/reference/configuration",editUrl:"https://github.com/starlake-ai/starlake/edit/master/docs/docs/reference/configuration.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Configuration"},sidebar:"cometSidebar",previous:{title:"Example",permalink:"/starlake/docs/userguide/sample"},next:{title:"Environment",permalink:"/starlake/docs/reference/environment"}},m=[{value:"Configuration",id:"configuration",children:[{value:"application.conf",id:"applicationconf",children:[]},{value:"Environment variables",id:"environment-variables",children:[]}]},{value:"Configuration sections",id:"configuration-sections",children:[{value:"Filesystem",id:"filesystem",children:[]},{value:"Ingestion",id:"ingestion",children:[]},{value:"Validation",id:"validation",children:[]},{value:"Privacy",id:"privacy",children:[]},{value:"Sinks",id:"sinks",children:[]},{value:"Audit / Metrics / Assertions",id:"audit--metrics--assertions",children:[]},{value:"Elasticsearch",id:"elasticsearch",children:[]},{value:"Spark",id:"spark",children:[]},{value:"Kafka",id:"kafka",children:[]},{value:"JDBC",id:"jdbc",children:[]},{value:"\xb5-service",id:"\xb5-service",children:[]},{value:"Airflow",id:"airflow",children:[]}]},{value:"Airflow DAGs",id:"airflow-dags",children:[{value:"Import DAG",id:"import-dag",children:[]},{value:"Watch DAG",id:"watch-dag",children:[]},{value:"Ingestion DAG",id:"ingestion-dag",children:[]}]}],p={toc:m};function d(e){var t=e.components,a=(0,r.Z)(e,["components"]);return(0,i.kt)("wrapper",(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Comet Data Pipeline is written using Scala / Spark and is thus run using the spark-submit command."),(0,i.kt)("p",null,"To run it with the default configuration, you simply launch it as follows :"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"SPARK_HOME/bin/spark-submit --class com.ebiznext.comet.job.Main ../bin/comet-spark3_2.12-VERSION-assembly.jar COMMAND [ARGS]\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"SPARK_HOME: Spark home directory"),(0,i.kt)("li",{parentName:"ul"},"COMMAND: Any of the command described in the CLI section followed by optional arguments"),(0,i.kt)("li",{parentName:"ul"},"ARGS: Option list of command arguments")),(0,i.kt)("h2",{id:"configuration"},"Configuration"),(0,i.kt)("h3",{id:"applicationconf"},"application.conf"),(0,i.kt)("p",null,"You may also pass any Spark arguments as usual but also pass a custom ",(0,i.kt)("inlineCode",{parentName:"p"},"application.conf")," file\nusing the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/lightbend/config"},"HOCON")," syntax  that supersedes the default Comet Data Pipeline settings.\ndefault settings are found in the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/ebiznext/comet-data-pipeline/blob/master/src/main/resources/reference.conf"},"reference.conf"),"\nand ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/ebiznext/comet-data-pipeline/blob/master/src/main/resources"},"reference-*.conf")," files. In your ",(0,i.kt)("inlineCode",{parentName:"p"},"application.conf"),"file you only\nneed to redefine the variables you want to customize."),(0,i.kt)("p",null,"Some of those configurations may also be redefined through environment variables."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"In client mode: To pass those env vars, simply export / set them before calling spark-submmit."),(0,i.kt)("li",{parentName:"ul"},"In cluster mode, you need to pass them as extra driver options.")),(0,i.kt)("p",null,"Passing the ",(0,i.kt)("inlineCode",{parentName:"p"},"application.conf")," file to the spark job use the syntax below:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'export CUSTOM_OPTIONS="--conf spark.driver.extraJavaOptions=-Dconfig.file=$PWD/application.conf"\nSPARK_HOME/bin/spark-submit $CUSTOM_OPTIONS --class com.ebiznext.comet.job.Main ../bin/comet-spark3_2.12-VERSION-assembly.jar COMMAND [ARGS]\n')),(0,i.kt)("h3",{id:"environment-variables"},"Environment variables"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"On Premise: To pass Comet Data Pipeline env vars in cluster mode, you'll have to put them in the spark-defaults.conf file or pass them as arguments to your\nSpark job as described in this ",(0,i.kt)("a",{parentName:"p",href:"https://stackoverflow.com/questions/37887168/how-to-pass-environment-variables-to-spark-driver-in-cluster-mode-with-spark-sub"},"article"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"On Google Cloud: To make it available for all your jobs, you need to pass them in the ",(0,i.kt)("inlineCode",{parentName:"p"},"DataprocClusterCreateOperator")," using the ",(0,i.kt)("inlineCode",{parentName:"p"},"spark-env:"),"prefix\nas described in the example below:"))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'    create_cluster = DataprocClusterCreateOperator(\n        task_id=\'create_dataproc_cluster\',\n        cluster_name=CLUSTER_NAME,\n        num_workers= \'${dataproc_cluster_size}\',\n        zone=ZONE,\n        region="${region}",\n        tags = ["dataproc"],\n        storage_bucket = "dataproc-${project_id}",\n        image_version=\'2.0.1-debian10\',\n        master_machine_type=MASTER_MACHINE_TYPE,\n        worker_machine_type=WORKER_MACHINE_TYPE,\n        service_account = "${service_account}",\n        internal_ip_only = True,\n        subnetwork_uri = "projects/${project_id}/regions/${region}/subnetworks/${subnet}",\n        properties = {\n            "spark-env:COMET_FS": "gs://${my_bucket}",\n            "spark-env:COMET_HIVE": "false",\n            "spark-env:COMET_GROUPED": "false",\n            "spark-env:COMET_AUDIT_SINK_TYPE": "BigQuerySink"\n            }\n    )\n')),(0,i.kt)("p",null,"In the example above, the variables are available in all the tasks that will be started on this cluster."),(0,i.kt)("p",null,"To set variables for specific tasks only, use a syntax similar to this one:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"t1 = dataproc_operator.DataProcSparkOperator(\n  task_id ='my_task',\n  dataproc_spark_jars='gs://my-bucket/comet-spark3_2.12-VERSION-assembly.jar',\n  cluster_name='cluster',\n  main_class = 'com.ebiznext.comet.job.Main',\n  arguments=['import'],\n  project_id='my-project-id',\n  dataproc_spark_properties={'spark.driver.extraJavaOptions':'-DCOMET_FS=gs://${my_bucket} -DCOMET_HIVE=false -DCOMET_GROUPED=false'},\n  dag=dag)\n")),(0,i.kt)("h2",{id:"configuration-sections"},"Configuration sections"),(0,i.kt)("h3",{id:"filesystem"},"Filesystem"),(0,i.kt)("p",null,"A filesystem is the location where datasets and Comet Data Pipeline metadata used for ingestion are stored."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"On premise this reference the folder where datasets and metadata are stored, eq.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"On a local filesystem: file://"),(0,i.kt)("li",{parentName:"ul"},"On a HDFS: hdfs://localhost:9000"))),(0,i.kt)("li",{parentName:"ul"},"In the cloud:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"On Google Cloud Platform: gs://my-bucket"),(0,i.kt)("li",{parentName:"ul"},"On Microsoft Azure: abfs://",(0,i.kt)("a",{parentName:"li",href:"mailto:my-bucket@comet.dfs.core.windows.net"},"my-bucket@comet.dfs.core.windows.net")),(0,i.kt)("li",{parentName:"ul"},"On Amazon Web Service: s3a://my_bucket")))),(0,i.kt)("p",null,"By default, Comet expect metadata in the /tmp/metadata folder and will store ingested datasets in the /tmp/datasets folder.\nBelow is how the folders look like by default for the provided quickstart sample."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"    /tmp\n    |-- datasets (Root folder of ingested datasets)\n    |   |-- accepted (Root folder of all valid records)\n    |   |   |-- hr (domain name as specified in the name attribute of the /tmp/metadata/hr.yml)\n    |   |   |   `-- sellers (Schema name as specified in the /tmp/metadata/hr.yml)\n    |   |   |       |-- _SUCCESS\n    |   |   |       `-- part-00000-292c081b-7291-4797-b935-17bc9409b03b.snappy.parquet\n    |   |   `-- sales\n    |   |       |-- customers (valid records for this schema as specified in the /tmp/metadata/sales.yml)\n    |   |       |   |-- _SUCCESS\n    |   |       |   `-- part-00000-562501a1-34ef-4b94-b527-8e93bcbb5f89.snappy.parquet\n    |   |       `-- orders (valid records for this schema as specified in the /tmp/metadata/sales.yml)\n    |   |           |-- _SUCCESS\n    |   |           `-- part-00000-92544093-4ae2-4a98-8df8-a5aba19a1b27.snappy.parquet\n    |   |-- archive (Source files as found in the incoming folder are saved here after processing)\n    |   |   |-- hr (Domain name)\n    |   |   |   `-- sellers-2018-01-01.json\n    |   |   `-- sales\n    |   |       |-- customers-2018-01-01.psv\n    |   |       `-- orders-2018-01-01.csv\n    |   |-- business\n    |   |   |-- hr\n    |   |   `-- sales\n    |   |-- metrics\n    |   |   |-- discrete\n    |   |   |-- continuous\n    |   |   `-- frequencies\n    |   |-- ingesting (Temporary folder used during ingestion by Comet)\n    |   |   |-- hr (One temporary subfolder / domain)\n    |   |   `-- sales\n    |   |-- pending (Source files are copied here from the incoming folder before processing)\n    |   |   |-- hr (one folder / domain)\n    |   |   `-- sales\n    |   |-- rejected (invalid records in processed datasets are stored here)\n    |   |   |-- hr (Domain name)\n    |   |   |   `-- sellers (Schema name)\n    |   |   |       |-- _SUCCESS\n    |   |   |       `-- part-00000-aef2dde6-af24-4e20-ad88-3e5238916e57.snappy.parquet\n    |   |   `-- sales\n    |   |       |-- customers\n    |   |       |   |-- _SUCCESS\n    |   |       |   `-- part-00000-e6fa5ff9-ad29-4e5f-a5ff-549dd331fafd.snappy.parquet\n    |   |       `-- orders\n    |   |           |-- _SUCCESS\n    |   |           `-- part-00000-6f7ba5d4-960b-4ac6-a123-87a7ab2d212f.snappy.parquet\n    |   `-- unresolved (Files found in the incoming folder but do not match any schema)\n    |       `-- hr\n    |           `-- dummy.json\n    `-- metadata (Root of metadata files)\n        |-- domains (all domain definition files are located in this folder)\n        |   |-- hr.yml (One definition file / domain)\n        |   `-- sales.yml\n        `-- assertions (All assertion definitions go here)\n        |   |-- default.comet.yml (Predefined assertion definitions)\n        |   `-- assertions.comet.yml (assertion definitions defined here are accessible throughout the project)\n        `-- views (All views definitions go here)\n        |   |-- default.comet.yml (Predefined view definitions)\n        |   `-- views.comet.yml (view definitions defined here are accessible throughout the project)\n        `-- types (All semantic types are defined here)\n        |   |-- default.comet.yml (Default semantic types)\n        |   `-- types.comet.yml (User defined semantic types, overwrite default ones)\n        `-- jobs (All transform jobs go here)\n            `-- sales-by-name.yml (Compute sales by )\n")),(0,i.kt)("p",null,"Comet Data Pipeline allows you to store datasets and metadata in two different filesystems. Thi is useful if you want to define a specific lifecycle\nfor your datasets.\nAlmost all options are customizable through environnement variables.\nThe main env vars are described below, you may change default settings. The exhaustive list of predefined env vars can be found in the reference.conf file."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"left"},"HOCON Variable"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Env variable"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"file-system"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_FS"),(0,i.kt)("td",{parentName:"tr",align:"left"},"file://"),(0,i.kt)("td",{parentName:"tr",align:"left"},"File system where datasets will be located")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"metadata-file-system"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_METADATA_FS"),(0,i.kt)("td",{parentName:"tr",align:"left"},"${file-system}"),(0,i.kt)("td",{parentName:"tr",align:"left"},"File system where Comet metadata will be located")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"root"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_ROOT"),(0,i.kt)("td",{parentName:"tr",align:"left"},"/tmp"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Root directory of the datasets and metadata files in the defined filesystem above")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"tmp-dir"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_TMPDIR"),(0,i.kt)("td",{parentName:"tr",align:"left"},"When compacting data and estimating number of partitions, Comet stores intermediates files in this folder"),(0,i.kt)("td",{parentName:"tr",align:"left"},'${root}"/comet_tmp"')),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"datasets"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_DATASETS"),(0,i.kt)("td",{parentName:"tr",align:"left"},'${root}"/datasets"'),(0,i.kt)("td",{parentName:"tr",align:"left"},"Folder where datasets are located in the datasets ",(0,i.kt)("inlineCode",{parentName:"td"},"file-system"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"metadata"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_METADATA"),(0,i.kt)("td",{parentName:"tr",align:"left"},'${root}"/metadata" otherwise'),(0,i.kt)("td",{parentName:"tr",align:"left"},"Folder where metadata are located in the metadata ",(0,i.kt)("inlineCode",{parentName:"td"},"metadata-file-system"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"file-system"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_FS"),(0,i.kt)("td",{parentName:"tr",align:"left"},"file://"),(0,i.kt)("td",{parentName:"tr",align:"left"},"File system where datasets will be located")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"area.pending"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_AREA_PENDING"),(0,i.kt)("td",{parentName:"tr",align:"left"},"pending"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Source files are copied here from the incoming folder before processing")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"area.unresolved"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_AREA_UNRESOLVED"),(0,i.kt)("td",{parentName:"tr",align:"left"},"unresolved"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Files found in the incoming folder but do not match any schema")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"area.archive"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_AREA_ARCHIVE"),(0,i.kt)("td",{parentName:"tr",align:"left"},"archive"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Source files as found in the incoming folder are saved here after processing")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"area.ingesting"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_AREA_INGESTING"),(0,i.kt)("td",{parentName:"tr",align:"left"},"ingesting"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Temporary folder used during ingestion by Comet")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"area.accepted"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_AREA_ACCEPTED"),(0,i.kt)("td",{parentName:"tr",align:"left"},"accepted"),(0,i.kt)("td",{parentName:"tr",align:"left"},"root folder of all valid records")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"area.rejected"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_AREA_REJECTED"),(0,i.kt)("td",{parentName:"tr",align:"left"},"rejected"),(0,i.kt)("td",{parentName:"tr",align:"left"},"invalid records in processed datasets are stored here")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"area.business"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_AREA_BUSINESS"),(0,i.kt)("td",{parentName:"tr",align:"left"},"business"),(0,i.kt)("td",{parentName:"tr",align:"left"},"root folder for all datasets produced by autojobs")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"archive"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_ARCHIVE"),(0,i.kt)("td",{parentName:"tr",align:"left"},"true"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Should we archive the incoming files once they are ingested")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"default-write-format"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_DEFAULT_WRITE_FORMAT"),(0,i.kt)("td",{parentName:"tr",align:"left"},"parquet"),(0,i.kt)("td",{parentName:"tr",align:"left"},"How accepted records are stored (parquet / orc / json / csv / avro)")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"default-rejected-write-format"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_DEFAULT_REJECTED_WRITE_FORMAT"),(0,i.kt)("td",{parentName:"tr",align:"left"},"parquet"),(0,i.kt)("td",{parentName:"tr",align:"left"},"How rejected records are stored (parquet / orc / json / csv / avro)")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"default-audit-write-format"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_DEFAULT_AUDIT_WRITE_FORMAT"),(0,i.kt)("td",{parentName:"tr",align:"left"},"parquet"),(0,i.kt)("td",{parentName:"tr",align:"left"},"How audit is stored (parquet / orc / json / csv / avro)")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"hive"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_HIVE"),(0,i.kt)("td",{parentName:"tr",align:"left"},"true"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Should we create external Hive tables for ingested files ?")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"analyze"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_ANALYZE"),(0,i.kt)("td",{parentName:"tr",align:"left"},"true"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Should we computed basic statistics ? (requires COMET_HIVE to be set to true)")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"launcher"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_LAUNCHER"),(0,i.kt)("td",{parentName:"tr",align:"left"},"simple"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Which orchestrator to use ? Valid values are airflow or simple (direct call)")))),(0,i.kt)("p",null,"Below is the default YAML file for filesystem options:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-hocon"},'file-system = "file://"\nfile-system = ${?COMET_FS}\n\nmetadata-file-system = ${file-system}\nmetadata-file-system = ${?COMET_METADATA_FS}\n\n\nroot = "/tmp"\nroot = ${?COMET_ROOT}\n\ndatasets = ${root}"/datasets"\ndatasets = ${?COMET_DATASETS}\n\nmetadata = ${root}"/metadata"\nmetadata = ${?COMET_METADATA}\n\narea {\n  pending = "pending"\n  pending = ${?COMET_AREA_PENDING}\n  unresolved = "unresolved"\n  unresolved = ${?COMET_AREA_UNRESOLVED}\n  archive = "archive"\n  archive = ${?COMET_AREA_ARCHIVE}\n  ingesting = "ingesting"\n  ingesting = ${?COMET_AREA_INGESTING}\n  accepted = "accepted"\n  accepted = ${?COMET_AREA_ACCEPTED}\n  rejected = "rejected"\n  rejected = ${?COMET_AREA_REJECTED}\n  business = "business"\n  business = ${?COMET_AREA_BUSINESS}\n}\n\ntmpdir = ${root}"/comet_tmp"\ntmpdir = ${?COMET_TMPDIR}\n\narchive = true\narchive = ${?COMET_ARCHIVE}\n\ndefault-write-format = parquet\ndefault-write-format = ${?COMET_DEFAULT_WRITE_FORMAT}\n\ndefault-rejected-write-format = parquet\ndefault-rejected-write-format = ${?COMET_DEFAULT_REJECTED_WRITE_FORMAT}\n\ndefault-audit-write-format = parquet\ndefault-audit-write-format = ${?COMET_DEFAULT_AUDIT_WRITE_FORMAT}\n\nlauncher = airflow\nlauncher = simple\nlauncher = ${?COMET_LAUNCHER}\n')),(0,i.kt)("p",null,"To make sure, the same schema is not ingested by two concurrent Comet processes, Comet Data Pipeline uses a file lock when necessary."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"left"},"HOCON Variable"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Env variable"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"lock.path"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_LOCK_PATH"),(0,i.kt)("td",{parentName:"tr",align:"left"},'${root}"/locks"'),(0,i.kt)("td",{parentName:"tr",align:"left"},"Root folder where lock file is created")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"lock.timeout"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_LOCK_TIMEOUT"),(0,i.kt)("td",{parentName:"tr",align:"left"},"-1"),(0,i.kt)("td",{parentName:"tr",align:"left"},"How long to wait for the file lock to be available (in seconds)")))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-hocon"},'lock {\n  path = ${root}"/locks"\n  path = ${?COMET_LOCK_PATH}\n\n  timeout = -1\n  timeout = ${?COMET_LOCK_TIMEOUT}\n}\n')),(0,i.kt)("h3",{id:"ingestion"},"Ingestion"),(0,i.kt)("p",null,"When many files that have the same pattern and thus belong to the same schema, it is possible to ingest them one after the other using an ingestion policy\nor ingest all of them at once."),(0,i.kt)("p",null,"When ingesting the files with the same schema one after the other, it is possible to use a custom ordering policy by settings the ",(0,i.kt)("inlineCode",{parentName:"p"},"COMET_LOAD_STRATEGY")," environment variable. Currently, the following ordering policies are defined:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"com.ebiznext.comet.job.load.IngestionTimeStrategy")," : Order the files by modification date"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"com.ebiznext.comet.job.load.IngestionNameStrategy")," : Order  the files by name")),(0,i.kt)("p",null,"If you want to use another custom strategy, you'll have to implement the trait below, make it available in the classpath and set the ",(0,i.kt)("inlineCode",{parentName:"p"},"COMET_LOAD_STRATEGY")," environment variable"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},'package com.ebiznext.comet.job.load\n\nimport java.time.LocalDateTime\n\nimport org.apache.hadoop.fs.{FileSystem, Path}\n\ntrait LoadStrategy {\n\n  /** List all files in folder\n    *\n    * @param fs        FileSystem\n    * @param path      Absolute folder path\n    * @param extension Files should end with this string. To list all files, simply provide an empty string\n    * @param since     Minimum modification time of list files. To list all files, simply provide the beginning of all times\n    * @param recursive List files recursively\n    * @return List of Path\n    */\n  def list(\n    fs: FileSystem,\n    path: Path,\n    extension: String = "",\n    since: LocalDateTime = LocalDateTime.MIN,\n    recursive: Boolean\n  ): List[Path]\n}\n')),(0,i.kt)("p",null,"To ingest all the files at once, set the ",(0,i.kt)("inlineCode",{parentName:"p"},"COMET_GROUPED")," variable to true."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"left"},"HOCON Variable"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Env variable"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"grouped"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_GROUPED"),(0,i.kt)("td",{parentName:"tr",align:"left"},"false"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Should files with the same schema be ingested all at once ?")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"load-strategy-class"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_LOAD_STRATEGY"),(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.job.load.IngestionTimeStrategy"),(0,i.kt)("td",{parentName:"tr",align:"left"},"When ",(0,i.kt)("inlineCode",{parentName:"td"},"grouped")," is false, which ingestion order strategy to use")))),(0,i.kt)("p",null,"Below is an example of HOCON file with the default values."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-hocon"},'load-strategy-class = "com.ebiznext.comet.job.load.IngestionTimeStrategy"\nload-strategy-class = ${?COMET_LOAD_STRATEGY}\n\ngrouped = false\ngrouped = ${?COMET_GROUPED}\n')),(0,i.kt)("p",null,"The YAML file describing the schema and ingestion rules may also define a custom sink (FS / JDBC / BigQuery / Redshift ...). "),(0,i.kt)("p",null,"In test mode, we need to sink the files to the filesystem. To enable sinking the resulting parquet file even when another sink type is desired, simply\nset the ",(0,i.kt)("inlineCode",{parentName:"p"},"COMET_SINK_TO_FILE")," environment variable to ",(0,i.kt)("inlineCode",{parentName:"p"},"true"),"."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"left"},"HOCON Variable"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Env variable"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"sink-to-file"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_SINK_TO_FILE"),(0,i.kt)("td",{parentName:"tr",align:"left"},"false"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Should ingested files be stored on the filesystem on only in the sink defined in the YAML file ?")))),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-hocon"},"sink-to-file = false\nsink-to-file = ${?COMET_SINK_TO_FILE}\n")),(0,i.kt)("p",null,"When ",(0,i.kt)("inlineCode",{parentName:"p"},"sink to file")," or a filesystem sink (SinkType.FS) is requested, and you want to output the result in a single file in the csv file format, set the ",(0,i.kt)("inlineCode",{parentName:"p"},"COMET_CSV_OUTPUT"),"\nenvironment variable to ",(0,i.kt)("inlineCode",{parentName:"p"},"true"),"."),(0,i.kt)("h3",{id:"validation"},"Validation"),(0,i.kt)("p",null,"During ingestion, the input file is validated up to the attribute level. Three default row validators are defined:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"com.ebiznext.comet.job.validator.FlatRowValidator: to validate flat files, eq. DSV, Position and single level Json files."),(0,i.kt)("li",{parentName:"ul"},"com.ebiznext.comet.job.validator.TreeRowValidator:  used for tree like documents, eq. XML and JSON files"),(0,i.kt)("li",{parentName:"ul"},"com.ebiznext.comet.job.validator.AcceptAllValidator: used for any document type (flat and tree like) and accept the input without any validation")),(0,i.kt)("p",null,"The validtor to use is configurable as follows:"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"left"},"HOCON Variable"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Env. variable"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Default value"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"row-validator-class"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_ROW_VALIDATOR_CLASS"),(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.job.validator.FlatRowValidator")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"tree-validator-class"),(0,i.kt)("td",{parentName:"tr",align:"left"},"COMET_TREE_VALIDATOR_CLASS"),(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.job.validator.TreeRowValidator")))),(0,i.kt)("h3",{id:"privacy"},"Privacy"),(0,i.kt)("p",null,"Default valid values are NONE, HIDE, MD5, SHA1, SHA256, SHA512, AES(not implemented).\nCustom values may also be defined by adding a new privacy option in the application.conf.\nThe default reference.conf file defines the following valid privacy strategies:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-hocon"},'privacy {\n  options = {\n    "none": "com.ebiznext.comet.privacy.No",\n    "hide": "com.ebiznext.comet.privacy.Hide",\n    "hide10X": "com.ebiznext.comet.privacy.Hide(\\"X\\",10)",\n    "approxLong20": "com.ebiznext.comet.privacy.ApproxLong(20)",\n    "md5": "com.ebiznext.comet.privacy.Md5",\n    "sha1": "com.ebiznext.comet.privacy.Sha1",\n    "sha256": "com.ebiznext.comet.privacy.Sha256",\n    "sha512": "com.ebiznext.comet.privacy.Sha512",\n    "initials": "com.ebiznext.comet.privacy.Initials"\n  }\n}\n')),(0,i.kt)("p",null,"In the YAML file, reference, you reference the option name. This will apply the function defined in the class referenced by the option value."),(0,i.kt)("p",null,"Below the predefined strategies:"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"left"},"Privacy Strategy"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Privacy class"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"none"),(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.No"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return the input string itself")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"hide"),(0,i.kt)("td",{parentName:"tr",align:"left"},'com.ebiznext.comet.privacy.Hide(\\"X\\", 10)'),(0,i.kt)("td",{parentName:"tr",align:"left"},"Without a parameter, return the empty string. Otherwise, replace with 10 occurrences of the character 'X'")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"md5"),(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.Md5"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return the md5 of the input string")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"sha1"),(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.Sha1"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return the sha1 of the input string")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"sha256"),(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.Sha256"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return the sha256 of the input string")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"sha512"),(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.Sha512"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return the sha256 of the input string")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"initials"),(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.Initials"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return the first char of each word (usually applied to user names)")))),(0,i.kt)("p",null,"The following startegies are also defined and may be declared in the custom configuration file."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"left"},"Privacy class"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.IPv4(8)"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return the IPv4 address with the last 8 bytes masked")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.IPv6(8"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return the IPv6 address with the last 8 bytes masked")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.RandomDouble"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return a random double number")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.RandomDouble(10,20)"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return a random double between 10.0 and 20.0")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.RandomLong"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return a random long number")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.RandomLong(10, 20)"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return a random long number between 10 and 20")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.RandomInt"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return a random int number")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.RandomInt(10, 20)"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return a random int number between 10 and 20")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.ApproxDouble(70)"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return a double value with a variation up to 70% applied to the input value")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"com.ebiznext.comet.privacy.ApproxLong(70)"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Return a double long with a variation up to 70% applied to the input value")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},'com.ebiznext.comet.privacy.Mask(\\"*\\", 4, 1, 3)'),(0,i.kt)("td",{parentName:"tr",align:"left"},"Partially mask the input value with 4 occurrences of the '*' character, 1 on the left side and 3 on the right side.")))),(0,i.kt)("p",null,"Any new privacy strategy should implement the following trait :"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"/** @param s: String  => Input string to encrypt\n  * @param colMap : Map[String, Option[String]] => Map of all the attributes and their corresponding values\n  * @param params: List[Any]  => Parameters passed to the algorithm as defined in the conf file.\n  *                               Parameter starting with '\"' is converted to a string\n  *                               Parameter containing a '.' is converted to a double\n  *                               Parameter equals to true of false is converted a boolean\n  *                               Anything else is converted to an int\n  * @return The encrypted string\n  */\n")),(0,i.kt)("h3",{id:"sinks"},"Sinks"),(0,i.kt)("h4",{id:"bigquery-sink"},"BigQuery Sink"),(0,i.kt)("p",null,"When type field is set to ",(0,i.kt)("inlineCode",{parentName:"p"},"BigQuerySink")),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"left"},"Property"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Type"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"name"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Optional String"),(0,i.kt)("td",{parentName:"tr",align:"left"},"None"),(0,i.kt)("td",{parentName:"tr",align:"left"})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"location"),(0,i.kt)("td",{parentName:"tr",align:"left"},"String"),(0,i.kt)("td",{parentName:"tr",align:"left"},"EU"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Database location (EU, US, ...)")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"timestamp"),(0,i.kt)("td",{parentName:"tr",align:"left"},"String"),(0,i.kt)("td",{parentName:"tr",align:"left"},"None"),(0,i.kt)("td",{parentName:"tr",align:"left"},"The timestamp column to use for table partitioning if any. No partitioning by default")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"clustering"),(0,i.kt)("td",{parentName:"tr",align:"left"},"List"),(0,i.kt)("td",{parentName:"tr",align:"left"},"None"),(0,i.kt)("td",{parentName:"tr",align:"left"},"List of ordered columns to use for table clustering")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"days"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Int"),(0,i.kt)("td",{parentName:"tr",align:"left"},"None"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Number of days before this table is set as expired and deleted. Never by default.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"requirePartitionFilter"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Boolean"),(0,i.kt)("td",{parentName:"tr",align:"left"},"false"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Should be require a partition filter on every request ? No by default.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"options"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Map"),(0,i.kt)("td",{parentName:"tr",align:"left"},"None"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Spark or BigQuery (depend on the selected engine) options to be set on the BigQuery connection")))),(0,i.kt)("h4",{id:"elasticsearch-sink"},"Elasticsearch Sink"),(0,i.kt)("p",null,"When type field is set to ",(0,i.kt)("inlineCode",{parentName:"p"},"EsSink")),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"left"},"Property"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Type"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"name"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Optional String"),(0,i.kt)("td",{parentName:"tr",align:"left"},"None"),(0,i.kt)("td",{parentName:"tr",align:"left"})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"id"),(0,i.kt)("td",{parentName:"tr",align:"left"},"String"),(0,i.kt)("td",{parentName:"tr",align:"left"},"None"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Attribute to use as id of the document. Generated by Elasticseach if not specified.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"timestamp"),(0,i.kt)("td",{parentName:"tr",align:"left"},"String"),(0,i.kt)("td",{parentName:"tr",align:"left"},"None"),(0,i.kt)("td",{parentName:"tr",align:"left"},'Timestamp field format as expected by Elasticsearch ("{beginTs',"|",'yyyy.MM.dd}" for example).')),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"options"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Map"),(0,i.kt)("td",{parentName:"tr",align:"left"},"None"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Elasticsearch options to be set on the ES connection")))),(0,i.kt)("h4",{id:"filesystem-sink"},"Filesystem Sink"),(0,i.kt)("p",null,"When type field is set to ",(0,i.kt)("inlineCode",{parentName:"p"},"FsSink"),". FsSink est the default sink type when ingesting data.\nThe file where data is saved is computed using the domain and schema name. See ",(0,i.kt)("a",{parentName:"p",href:"/starlake/docs/howto/load"},"Load")," and ",(0,i.kt)("a",{parentName:"p",href:"/starlake/docs/howto/transform"},"Transform")),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"left"},"Property"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Type"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"name"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Optional String"),(0,i.kt)("td",{parentName:"tr",align:"left"},"None"),(0,i.kt)("td",{parentName:"tr",align:"left"})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"connection"),(0,i.kt)("td",{parentName:"tr",align:"left"},"String"),(0,i.kt)("td",{parentName:"tr",align:"left"},"None"),(0,i.kt)("td",{parentName:"tr",align:"left"},"JDBC Connection String")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},"options"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Map"),(0,i.kt)("td",{parentName:"tr",align:"left"},"None"),(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("a",{parentName:"td",href:"https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html"},"JDBC Options"))))),(0,i.kt)("h3",{id:"audit--metrics--assertions"},"Audit / Metrics / Assertions"),(0,i.kt)("h3",{id:"elasticsearch"},"Elasticsearch"),(0,i.kt)("h3",{id:"spark"},"Spark"),(0,i.kt)("h3",{id:"kafka"},"Kafka"),(0,i.kt)("h3",{id:"jdbc"},"JDBC"),(0,i.kt)("h3",{id:"\xb5-service"},"\xb5-service"),(0,i.kt)("h3",{id:"airflow"},"Airflow"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"left"},"HOCON Variable"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Env variable"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Default Value"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"}),(0,i.kt)("td",{parentName:"tr",align:"left"},"AIRFLOW_ENDPOINT"),(0,i.kt)("td",{parentName:"tr",align:"left"},"Airflow endpoint. Used when COMET_LAUNCHER is set to airflow"),(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("a",{parentName:"td",href:"http://127.0.0.1:8080/api/experimental"},"http://127.0.0.1:8080/api/experimental"))))),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Env. Var"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"),(0,i.kt)("th",{parentName:"tr",align:null},"Default value")))),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"\ud83d\udcdd ",(0,i.kt)("strong",{parentName:"p"},"When running Spark on YARN in cluster mode,\nenvironment variables need to be set using the syntax spark.yarn.appMasterEnv.","[EnvironmentVariableName]"))),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"\ud83d\udcdd ",(0,i.kt)("strong",{parentName:"p"},'When running Dataproc on GCP, environment variables need to be set\nin the DataprocClusterCreateOperator in the properties attributes\nusing the syntax "spark-env:',"[EnvironmentVariableName]",'":"',"[Value]",'"'))),(0,i.kt)("h2",{id:"airflow-dags"},"Airflow DAGs"),(0,i.kt)("p",null,"Comet Data Pipeline comes with native  Airflow support.\nBelow are DAG definitions for each of the three ingestion steps on an kerberized cluster."),(0,i.kt)("h3",{id:"import-dag"},"Import DAG"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\n\n\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2018, 11, 2),\n    'email': ['me@here.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5),\n\n}\n\ndag = DAG('comet_import',max_active_runs=1, catchup=False, default_args=default_args, schedule_interval='*/1 * * * *')\n\n\n\nCOMET_SPARK_CMD = \"spark2-submit \\\n                        --keytab /etc/keytabs/importhdfs.keytab \\\n                        --principal importhdfs@MY.BIGDATA \\\n                        --conf spark.jars.packages=\\\"\\\" \\\n                        --master yarn \\\n                        --deploy-mode client /home/airflow/program/comet-assembly-0.1.jar\"\n\nCometImport = BashOperator(\n    task_id='comet_import',\n    bash_command= COMET_SPARK_CMD + ' import',\n    env={\n        'COMET_DATASETS':\"/project/data\",\n        'COMET_METADATA':\"/project/metadata\",\n        'COMET_AREA_ACCEPTED':\"working\",\n        'COMET_AREA_PENDING':\"staging\",\n        'COMET_ARCHIVE':\"true\",\n        'COMET_LAUNCHER':\"airflow\",\n        'COMET_HIVE':\"true\",\n        'COMET_ANALYZE':\"true\"\n    },\n    dag=dag)\n")),(0,i.kt)("h3",{id:"watch-dag"},"Watch DAG"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import os\nfrom airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime, timedelta\nfrom airflow.operators.slack_operator import SlackAPIPostOperator\n\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2018, 11, 2),\n    'email': ['me@here.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5),\n    # 'queue': 'bash_queue',\n    # 'pool': 'backfill',\n    # 'priority_weight': 10,\n    # 'end_date': datetime(2016, 1, 1),\n}\n\ndag = DAG('comet_watcher',max_active_runs=1 , catchup=False, default_args=default_args, schedule_interval='*/1 * * * *')\n\ndef slack_task(msg):\n    slack_alert = SlackAPIPostOperator(\n        task_id='slack_alert',\n        channel=\"#airflow\",\n        token=\"xoxp-64071012534-475450904118-524897638692-f9a90d49fd7fb312a574b4570d557b9a\",\n        text = msg,\n        username = 'airflow',)\n    return slack_alert.execute(msg=msg)\n\nCOMET_SPARK_CMD = \"spark2-submit \\\n                        --keytab /etc/keytabs/importhdfs.keytab \\\n                        --principal importhdfs@MY.BIGDATA \\\n                        --conf spark.jars.packages=\\\"\\\" \\\n                        --master yarn \\\n                        --deploy-mode client /home/airflow/program/comet-assembly-0.1.jar\"\n\nCOMET_DOMAIN = os.environ.get('COMET_DOMAIN', '')\nCometWatch = BashOperator(\n    task_id='comet_watcher',\n    bash_command= COMET_SPARK_CMD + ' watch '+ COMET_DOMAIN,\n    #on_failure_callback=slack_task(\":red_circle: Task Comet Watch Failed\"),\n    #on_success_callback=slack_task(\":ok_hand: Task Comet Watch Success\"),\n    env={\n        'AIRFLOW_ENDPOINT':\"https://airflow.my.server.com/api/experimental\",\n        'COMET_DATASETS':\"/project/data\",\n        'COMET_METADATA':\"/project/metadata\",\n        'COMET_AREA_ACCEPTED':\"working\",\n        'COMET_AREA_PENDING':\"staging\",\n        'COMET_ARCHIVE':\"true\",\n        'COMET_LAUNCHER':\"airflow\",\n        'COMET_HIVE':\"true\",\n        'COMET_ANALYZE':\"true\"\n    },\n    dag=dag)\n")),(0,i.kt)("h3",{id:"ingestion-dag"},"Ingestion DAG"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime, timedelta\nfrom airflow.operators.slack_operator import SlackAPIPostOperator\n\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2018, 11, 2),\n    'email': ['me@here.com'],\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 0,\n    'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG('comet_ingest',max_active_runs=1 , catchup=False, default_args=default_args, schedule_interval = None)\n\ndef slack_task(msg):\n    slack_alert = SlackAPIPostOperator(\n        task_id='slack_alert',\n        channel=\"#airflow\",\n        token=\"xoxp-64071012534-475450904118-524897638692-f9a90d49fd7fb312a574b4570d557b9a\",\n        text = msg,\n        username = 'airflow',)\n    return slack_alert.execute(msg=msg)\n\nCOMET_SPARK_CMD = \"spark2-submit \\\n                        --keytab /etc/keytabs/importhdfs.keytab \\\n                        --principal importhdfs@MY.BIGDATA \\\n                        --conf spark.jars.packages=\\\"\\\" \\\n                        --conf spark.yarn.appMasterEnv.COMET_METADATA=/project/metadata \\\n                        --conf spark.yarn.appMasterEnv.COMET_ACCEPTED=working \\\n                        --conf spark.yarn.appMasterEnv.COMET_DATASETS=/project/data \\\n                        --master yarn \\\n                        --deploy-mode cluster /home/airflow/program/comet-assembly-0.1.jar\"\n\ntemplated_command = COMET_SPARK_CMD + \"\"\" {{ dag_run.conf['command'] }}\"\"\"\n\nCometIngest = BashOperator(\n    task_id='comet_ingest',\n    bash_command=templated_command,\n    #on_failure_callback=slack_task(\":red_circle: Task Comet Ingest Failed: \"),\n    #on_success_callback=slack_task(\":ok_hand: Task Comet Ingest Success: \"),\n    env={\n        'COMET_DATASETS':\"/project/data\",\n        'COMET_METADATA':\"/project/metadata\",\n        'COMET_AREA_ACCEPTED':\"working\",\n        'COMET_AREA_PENDING':\"staging\",\n        'COMET_ARCHIVE':\"true\",\n        'COMET_LAUNCHER':\"airflow\",\n        'COMET_HIVE':\"true\",\n        'COMET_ANALYZE':\"true\"\n    },\n    dag=dag)\n")))}d.isMDXComponent=!0}}]);