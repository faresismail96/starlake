"use strict";(self.webpackChunkstarlake_docs=self.webpackChunkstarlake_docs||[]).push([[1986],{3905:function(e,t,r){r.d(t,{Zo:function(){return p},kt:function(){return m}});var a=r(7294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var u=a.createContext({}),l=function(e){var t=a.useContext(u),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},p=function(e){var t=l(e.components);return a.createElement(u.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,o=e.originalType,u=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=l(r),m=n,g=d["".concat(u,".").concat(m)]||d[m]||c[m]||o;return r?a.createElement(g,i(i({ref:t},p),{},{components:r})):a.createElement(g,i({ref:t},p))}));function m(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=r.length,i=new Array(o);i[0]=d;var s={};for(var u in t)hasOwnProperty.call(t,u)&&(s[u]=t[u]);s.originalType=e,s.mdxType="string"==typeof e?e:n,i[1]=s;for(var l=2;l<o;l++)i[l]=r[l];return a.createElement.apply(null,i)}return a.createElement.apply(null,r)}d.displayName="MDXCreateElement"},7705:function(e,t,r){r.r(t),r.d(t,{frontMatter:function(){return i},contentTitle:function(){return s},metadata:function(){return u},toc:function(){return l},default:function(){return c}});var a=r(7462),n=r(3366),o=(r(7294),r(3905)),i={sidebar_position:3,title:"Transform"},s=void 0,u={unversionedId:"examples/transform",id:"examples/transform",isDocsHomePage:!1,title:"Transform",description:"Parquet to Parquet",source:"@site/docs/examples/transform.md",sourceDirName:"examples",slug:"/examples/transform",permalink:"/starlake/docs/examples/transform",editUrl:"https://github.com/starlake-ai/starlake/edit/master/docs/docs/examples/transform.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,title:"Transform"},sidebar:"cometSidebar",previous:{title:"Load",permalink:"/starlake/docs/examples/load"},next:{title:"Microsoft Azure",permalink:"/starlake/docs/cloud/azure"}},l=[{value:"Parquet to Parquet",id:"parquet-to-parquet",children:[]},{value:"Transform Parquet to DSV",id:"transform-parquet-to-dsv",children:[]},{value:"Transform Parquet to BigQuery",id:"transform-parquet-to-bigquery",children:[]},{value:"BigQuery to BigQuery",id:"bigquery-to-bigquery",children:[]},{value:"BigQuery to CSV",id:"bigquery-to-csv",children:[]},{value:"BigQuery to Parquet",id:"bigquery-to-parquet",children:[]},{value:"Parquet to Elasticsearch",id:"parquet-to-elasticsearch",children:[]},{value:"BigQuery to Elasticsearch",id:"bigquery-to-elasticsearch",children:[]},{value:"BigQuery to SQL Database",id:"bigquery-to-sql-database",children:[]},{value:"Parquet to SQL Database",id:"parquet-to-sql-database",children:[]},{value:"SQL Database to SQL Database",id:"sql-database-to-sql-database",children:[]}],p={toc:l};function c(e){var t=e.components,r=(0,n.Z)(e,["components"]);return(0,o.kt)("wrapper",(0,a.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"parquet-to-parquet"},"Parquet to Parquet"),(0,o.kt)("p",null,"Will load the dataset ",(0,o.kt)("inlineCode",{parentName:"p"},"accepted/graduateProgram")," under ",(0,o.kt)("inlineCode",{parentName:"p"},"$COMET_DATASETS")," directory from the configured storage.\nAn absolute path may also be specified."),(0,o.kt)("p",null,"This example create two views : One temporary view in the ",(0,o.kt)("inlineCode",{parentName:"p"},"views")," section, and another one in the ",(0,o.kt)("inlineCode",{parentName:"p"},"presql")," section.\nNote that the sql request in the ",(0,o.kt)("inlineCode",{parentName:"p"},"presql")," section uses the view defined in the ",(0,o.kt)("inlineCode",{parentName:"p"},"views")," section."),(0,o.kt)("p",null,"The resulting file will be stored in the ",(0,o.kt)("inlineCode",{parentName:"p"},"$COMET_DATASETS/business/graduateProgram/output")," directory."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'---\ntransform:\n    name: "graduateProgram"\n    views:\n      graduate_View: "fs:accepted/graduateProgram"\n    tasks:\n      - domain: "graduateProgram"\n        area: "business"\n        dataset: "output"\n        write: "OVERWRITE"\n        presql: |\n          create or replace view graduate_agg_view\n          select degree,\n            department,\n            school\n          from graduate_View\n          where school={{school}}\n    \n        sql: SELECT * FROM graduate_agg_view\n')),(0,o.kt)("h2",{id:"transform-parquet-to-dsv"},"Transform Parquet to DSV"),(0,o.kt)("p",null,"Based ont the ",(0,o.kt)("a",{parentName:"p",href:"#parquet-to-parquet"},"parquet to parquet")," example, we add the format property to request a csv output\nand set coalesce to ",(0,o.kt)("inlineCode",{parentName:"p"},"true")," to output everything in a single CSV file."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'---\ntransform:\n    name: "graduateProgram"\n    format: "csv"\n    coalesce: true\n    views:\n      graduate_View: "fs:accepted/graduateProgram"\n    tasks:\n      - domain: "graduateProgram"\n        area: "business"\n        dataset: "output"\n        write: "OVERWRITE"\n        presql: |\n          create or replace view graduate_agg_view\n          select degree,\n            department,\n            school\n          from graduate_View\n          where school={{school}}\n    \n        sql: SELECT * FROM graduate_agg_view\n')),(0,o.kt)("h2",{id:"transform-parquet-to-bigquery"},"Transform Parquet to BigQuery"),(0,o.kt)("p",null,"Based ont the ",(0,o.kt)("a",{parentName:"p",href:"#parquet-to-parquet"},"parquet to parquet")," example, we add the sink section to force the task to store the SQL result in BigQuery"),(0,o.kt)("p",null,"The result will store in the current project under the ",(0,o.kt)("inlineCode",{parentName:"p"},"business")," BigQuery dataset in the ",(0,o.kt)("inlineCode",{parentName:"p"},"output")," table."),(0,o.kt)("p",null,"You may also specify the target project in the ",(0,o.kt)("inlineCode",{parentName:"p"},"/tasks/dataset")," property using the syntax ",(0,o.kt)("inlineCode",{parentName:"p"},"PROJECT_ID:business")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'---\ntransform:\n    name: "graduateProgram"\n    views:\n      graduate_View: "fs:accepted/graduateProgram"\n    tasks:\n      - domain: "graduateProgram"\n        area: "business"\n        dataset: "output"\n        write: "OVERWRITE"\n        sink:\n            type: BQ\n            location: EU\n        presql: |\n          create or replace view graduate_agg_view\n          select degree,\n            department,\n            school\n          from graduate_View\n          where school={{school}}\n    \n        sql: SELECT * FROM graduate_agg_view\n')),(0,o.kt)("h2",{id:"bigquery-to-bigquery"},"BigQuery to BigQuery"),(0,o.kt)("p",null,"We may use the Spark (SPARK) or BigQuery (BQ) engine. When using the BQ engine, no spark cluster is needed."),(0,o.kt)("p",null,"You may want to use the Spark engine if you need to run your jobs to stay agnostic to the underlying storage or\nif you need your jobs to overwrite only the partitions present in the resulting SQL."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'---\ntransform:\n    name: "graduateProgram"\n    views:\n      graduate_View: "bq:gcp_project_id:bqdataset/graduateProgram"\n    tasks:\n      - domain: "graduateProgram"\n        sink:\n            type: BQ\n        area: "business"\n        dataset: "output"\n        write: "OVERWRITE"\n        sql: SELECT * FROM graduate_View\n')),(0,o.kt)("h2",{id:"bigquery-to-csv"},"BigQuery to CSV"),(0,o.kt)("h2",{id:"bigquery-to-parquet"},"BigQuery to Parquet"),(0,o.kt)("h2",{id:"parquet-to-elasticsearch"},"Parquet to Elasticsearch"),(0,o.kt)("h2",{id:"bigquery-to-elasticsearch"},"BigQuery to Elasticsearch"),(0,o.kt)("h2",{id:"bigquery-to-sql-database"},"BigQuery to SQL Database"),(0,o.kt)("h2",{id:"parquet-to-sql-database"},"Parquet to SQL Database"),(0,o.kt)("h2",{id:"sql-database-to-sql-database"},"SQL Database to SQL Database"))}c.isMDXComponent=!0}}]);