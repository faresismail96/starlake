"use strict";(self.webpackChunkstarlake_docs=self.webpackChunkstarlake_docs||[]).push([[1193],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return d}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},l=Object.keys(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var o=a.createContext({}),u=function(e){var t=a.useContext(o),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=u(e.components);return a.createElement(o.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,l=e.originalType,o=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=u(n),d=r,k=m["".concat(o,".").concat(d)]||m[d]||c[d]||l;return n?a.createElement(k,i(i({ref:t},p),{},{components:n})):a.createElement(k,i({ref:t},p))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=n.length,i=new Array(l);i[0]=m;var s={};for(var o in t)hasOwnProperty.call(t,o)&&(s[o]=t[o]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var u=2;u<l;u++)i[u]=n[u];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},3610:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return i},contentTitle:function(){return s},metadata:function(){return o},toc:function(){return u},default:function(){return c}});var a=n(7462),r=n(3366),l=(n(7294),n(3905)),i={sidebar_position:30},s="Transform",o={unversionedId:"reference/transform",id:"reference/transform",isDocsHomePage:!1,title:"Transform",description:"Job",source:"@site/docs/reference/transform.md",sourceDirName:"reference",slug:"/reference/transform",permalink:"/starlake/docs/reference/transform",editUrl:"https://github.com/starlake-ai/starlake/edit/master/docs/docs/reference/transform.md",tags:[],version:"current",sidebarPosition:30,frontMatter:{sidebar_position:30},sidebar:"cometSidebar",previous:{title:"Load",permalink:"/starlake/docs/reference/load"},next:{title:"Extract",permalink:"/starlake/docs/howto/extract"}},u=[{value:"Job",id:"job",children:[]},{value:"Task",id:"task",children:[]},{value:"Partitioning",id:"partitioning",children:[]},{value:"Clustering",id:"clustering",children:[]},{value:"Views",id:"views",children:[]}],p={toc:u};function c(e){var t=e.components,n=(0,r.Z)(e,["components"]);return(0,l.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"transform"},"Transform"),(0,l.kt)("h2",{id:"job"},"Job"),(0,l.kt)("p",null,"A job is a set of transform tasks executed using the specified engine."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"name: String\n")),(0,l.kt)("p",null,(0,l.kt)("em",{parentName:"p"},"Required"),". Job logical name."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"tasks: List[Task]\n")),(0,l.kt)("p",null,(0,l.kt)("em",{parentName:"p"},"Required"),". List of transform tasks to execute."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"area : String\n")),(0,l.kt)("p",null,(0,l.kt)("em",{parentName:"p"},"Required"),". Area where the data is located."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"When using the BigQuery engine, teh area corresponds to the dataset name we will be working on in this job."),(0,l.kt)("li",{parentName:"ul"},'When using the Spark engine, this is folder where the data should be store. Default value is "business"')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"format: String\n")),(0,l.kt)("p",null,(0,l.kt)("em",{parentName:"p"},"Optional"),'. output file format when using Spark engine. Ingored for BigQuery. Default value is "parquet".'),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"coalesce: Boolean\n")),(0,l.kt)("p",null,(0,l.kt)("em",{parentName:"p"},"Optional"),". When outputting files, should we coalesce it to a single file. Useful when CSV is the output format."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"udf : String\n")),(0,l.kt)("p",null,(0,l.kt)("em",{parentName:"p"},"Optional"),"."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Register UDFs written in this JVM class when using Spark engine"),(0,l.kt)("li",{parentName:"ul"},"Register UDFs stored at this location when using BigQuery engine")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"views : Map[String,String]\n")),(0,l.kt)("p",null,(0,l.kt)("em",{parentName:"p"},"Optional"),". Create temporary views using where the key is the view name and the map the SQL request corresponding to this view using the SQL engine supported syntax."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"engine : String\n")),(0,l.kt)("p",null,(0,l.kt)("em",{parentName:"p"},"Optional"),". SPARK or BQ. Default value is SPARK."),(0,l.kt)("h2",{id:"task"},"Task"),(0,l.kt)("p",null,"Task executed in the context of a job. Each task is executed in its own session."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"sql: String\n")),(0,l.kt)("p",null,"Main SQL request to exexute (do not forget to prefix table names with the database name to avoid conflicts)"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"domain: String\n")),(0,l.kt)("p",null,"Output domain in output Area (Will be the Database name in Hive or Dataset in BigQuery)"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"dataset: String\n")),(0,l.kt)("p",null,"Dataset Name in output Area (Will be the Table name in Hive & BigQuery)"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"write: String\n")),(0,l.kt)("p",null,"Append to or overwrite existing data"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"area: String\n")),(0,l.kt)("p",null,"Target Area where domain / dataset will be stored."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"partition: List[String]\n")),(0,l.kt)("p",null,"List of columns used for partitioning the outtput."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"presql: List[String]\n")),(0,l.kt)("p",null,"List of SQL requests to executed before the main SQL request is run"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"postsql: List[String]\n")),(0,l.kt)("p",null,"List of SQL requests to executed after the main SQL request is run"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"sink: Sink\n")),(0,l.kt)("p",null,"Where to sink the data"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-scala"},"rls: List[RowLevelSecurity]\n")),(0,l.kt)("p",null,"Row level security policy to apply too the output data."),(0,l.kt)("h2",{id:"partitioning"},"Partitioning"),(0,l.kt)("h2",{id:"clustering"},"Clustering"),(0,l.kt)("p",null,(0,l.kt)("a",{parentName:"p",href:"https://deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/"},"https://deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/")),(0,l.kt)("h2",{id:"views"},"Views"))}c.isMDXComponent=!0}}]);